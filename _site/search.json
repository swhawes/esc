[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SPEP Revalidation",
    "section": "",
    "text": "Config OptionsInstall PackagesLoad DataPrep data\n\n\n\n\nThis code configures knitr code chunk options\n\n\n\nCode\nknitr::opts_chunk$set(\n    echo = T, message = F, warning = F, error = F,\n    comment = NA, cache = T, code_folding = T,\n    R.options = list(width = 220, digits = 3),\n    fig.align = \"center\",\n    out.width = \"75%\", fig.asp = .75\n)\n\n\n\n\n\n\n\nThis code loads the r packages necessary for this example\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(gtsummary)\nlibrary(DT)\nlibrary(patchwork)\nlibrary(dlookr)\nlibrary(kableExtra)\nlibrary(knitr)\nlibrary(openxlsx)\nlibrary(psych)\nlibrary(janitor)\n\n\n\n\n\n\nThis code loads the dataframe\n\n\n\nCode\n# Read the data from an Excel file\nmerged &lt;- read_excel(\"/Users/shawes/PA_JCMS/data/Merged_Files_NoRecid.xlsx\")\n\n\n\n\n\n\nThis code prepares the dataset for analysis\n\n\n\nCode\n## % Missingness by Variable\n\n# Calculate the percentage of missing data for each column\nmissing_data_percentages &lt;- merged %&gt;%\n  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"missing_percentage\")\n\n# Filter out columns with &gt;= 90% missing data\ncolumns_to_keep &lt;- missing_data_percentages %&gt;%\n  filter(missing_percentage &lt; 90) %&gt;%\n  pull(variable)\n\n# Select the columns to keep\nmerged &lt;- merged %&gt;%\n  select(all_of(columns_to_keep))\n\n\n\n\nCode\n## Prep data\n\n# List of sensitive variables to exclude\nsensitive_vars &lt;- c(\"mhs2_juvenile_id\", \"dob\", \"assessment_po\")\n## \"unique_id\", \"youth_num_in_cohort\"\n\n# Function to clean variable names\nclean_variable_names &lt;- function(merged) {\n  cleaned_names &lt;- merged %&gt;%\n    names() %&gt;%\n    str_replace_all(\"[^[:alnum:]_]\", \"_\") %&gt;%\n    str_replace_all(\"__+\", \"_\") %&gt;%\n    str_trim() %&gt;%\n    tolower()\n  \n  # Ensure unique names\n  cleaned_names &lt;- make.unique(cleaned_names)\n  \n  names(merged) &lt;- cleaned_names\n  return(merged)\n}\n\n# Function to calculate means and standard deviations for numeric columns\ncalculate_means_sds &lt;- function(merged) {\n  means_sds &lt;- sapply(merged, function(x) if(is.numeric(x)) {\n    mean_value &lt;- round(mean(x, na.rm = TRUE), 2)\n    sd_value &lt;- round(sd(x, na.rm = TRUE), 2)\n    paste0(mean_value, \" (\", sd_value, \")\")\n  } else NA)\n  return(means_sds)\n}\n\n# Function to set variable types based on dlookr diagnostics\nset_variable_types &lt;- function(merged, types) {\n  for (i in 1:nrow(types)) {\n    var &lt;- types$variables[i]\n    type &lt;- types$types[i]\n    if (type == \"numeric\") {\n      merged[[var]] &lt;- as.numeric(merged[[var]])\n    } else if (type == \"factor\") {\n      merged[[var]] &lt;- as.factor(merged[[var]])\n    } else if (type == \"integer\") {\n      merged[[var]] &lt;- as.integer(merged[[var]])\n    } else if (type == \"character\") {\n      merged[[var]] &lt;- as.character(merged[[var]])\n    }\n    # Add other types as necessary\n  }\n  return(merged)\n}\n\n# Function to convert character variables to factors\nconvert_char_to_factor &lt;- function(merged) {\n  merged &lt;- merged %&gt;%\n    mutate(across(where(is.character), as.factor))\n  return(merged)\n}\n\n# Function to calculate the mode for factor variables\ncalculate_modes &lt;- function(merged) {\n  modes &lt;- sapply(merged, function(x) if(is.factor(x)) {\n    mode_value &lt;- names(sort(table(x), decreasing = TRUE)[1])\n    mode_count &lt;- max(table(x))\n    mode_percentage &lt;- (mode_count / length(x)) * 100\n    paste0(mode_value, \" (\", round(mode_percentage, 2), \"%)\")\n  } else NA)\n  return(modes)\n}\n\n# Function to calculate date statistics for POSIXct columns\ncalculate_date_statistics &lt;- function(merged) {\n  date_stats &lt;- sapply(merged, function(x) if(inherits(x, \"POSIXct\")) {\n    min_date &lt;- format(min(x, na.rm = TRUE), \"%Y-%m-%d\")\n    max_date &lt;- format(max(x, na.rm = TRUE), \"%Y-%m-%d\")\n    num_unique_dates &lt;- length(unique(x))\n    median_date &lt;- format(median(x, na.rm = TRUE), \"%Y-%m-%d\")\n    paste0(min_date, \" to \", max_date)\n  } else NA)\n  return(date_stats)\n}\n\n# Function to count the number of levels for factor variables\ncount_levels &lt;- function(merged) {\n  num_levels &lt;- sapply(merged, function(x) if(is.factor(x)) length(levels(x)) else NA)\n  return(num_levels)\n}\n\n# Function to mask sensitive data, only for existing columns\nmask_sensitive_data &lt;- function(merged, sensitive_vars) {\n  existing_sensitive_vars &lt;- intersect(names(merged), sensitive_vars)\n  if(length(existing_sensitive_vars) &gt; 0) {\n    merged &lt;- merged %&gt;%\n      mutate(across(all_of(existing_sensitive_vars), ~ \"Masked\"))\n  }\n  return(merged)\n}\n\n# Apply the cleaning function to the dataset\nmerged_clean &lt;- clean_variable_names(merged)\n\n# Convert character variables to factors\n#merged_clean &lt;- convert_char_to_factor(merged_clean)\n\n# Determine variable types using dlookr\nvariable_types &lt;- diagnose(merged_clean) %&gt;%\n  select(variables, types)\n\n# Set variable types based on dlookr diagnostics\nmerged_clean &lt;- set_variable_types(merged_clean, variable_types)\n\n# Mask sensitive data\nmerged_clean &lt;- mask_sensitive_data(merged_clean, sensitive_vars)\n\n# Create a mapping table of original and cleaned names, excluding sensitive variables\nmapping_table &lt;- tibble(\n  original_name = names(merged),\n  cleaned_name = names(merged_clean)\n) %&gt;%\n  filter(!cleaned_name %in% sensitive_vars)\n\n# Calculate means and standard deviations for numeric columns\nmeans_sds &lt;- calculate_means_sds(merged_clean)\n\n# Calculate modes for factor columns\nmodes &lt;- calculate_modes(merged_clean)\n\n# Calculate date statistics for POSIXct columns\ndate_statistics &lt;- calculate_date_statistics(merged_clean)\n\n# Count the number of levels for factor variables\nnum_levels &lt;- count_levels(merged_clean)\n\n# Combine means, modes, date statistics, and sensitive summaries into separate columns\nmapping_table &lt;- mapping_table %&gt;%\n  mutate(mean_sd = means_sds[!names(means_sds) %in% sensitive_vars],\n         mode = modes[!names(modes) %in% sensitive_vars],\n         date_range = date_statistics[!names(date_statistics) %in% sensitive_vars],\n         num_levels = num_levels[!names(num_levels) %in% sensitive_vars]) %&gt;%\n  left_join(variable_types, by = c(\"cleaned_name\" = \"variables\"))\n\n# Ensure all numeric values in the table are rounded to two decimal places\nmapping_table &lt;- mapping_table %&gt;%\n  mutate(across(where(is.numeric), ~ round(., 2)))\n\n# Reorder the columns so that summary_statistic is before original_name\nmapping_table &lt;- mapping_table %&gt;%\n  select(cleaned_name, mean_sd, mode, date_range, num_levels, original_name, types)"
  },
  {
    "objectID": "index.html#preliminary-setup",
    "href": "index.html#preliminary-setup",
    "title": "SPEP Revalidation",
    "section": "",
    "text": "Config OptionsInstall PackagesLoad DataPrep data\n\n\n\n\nThis code configures knitr code chunk options\n\n\n\nCode\nknitr::opts_chunk$set(\n    echo = T, message = F, warning = F, error = F,\n    comment = NA, cache = T, code_folding = T,\n    R.options = list(width = 220, digits = 3),\n    fig.align = \"center\",\n    out.width = \"75%\", fig.asp = .75\n)\n\n\n\n\n\n\n\nThis code loads the r packages necessary for this example\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(gtsummary)\nlibrary(DT)\nlibrary(patchwork)\nlibrary(dlookr)\nlibrary(kableExtra)\nlibrary(knitr)\nlibrary(openxlsx)\nlibrary(psych)\nlibrary(janitor)\n\n\n\n\n\n\nThis code loads the dataframe\n\n\n\nCode\n# Read the data from an Excel file\nmerged &lt;- read_excel(\"/Users/shawes/PA_JCMS/data/Merged_Files_NoRecid.xlsx\")\n\n\n\n\n\n\nThis code prepares the dataset for analysis\n\n\n\nCode\n## % Missingness by Variable\n\n# Calculate the percentage of missing data for each column\nmissing_data_percentages &lt;- merged %&gt;%\n  summarise(across(everything(), ~ mean(is.na(.)) * 100)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"missing_percentage\")\n\n# Filter out columns with &gt;= 90% missing data\ncolumns_to_keep &lt;- missing_data_percentages %&gt;%\n  filter(missing_percentage &lt; 90) %&gt;%\n  pull(variable)\n\n# Select the columns to keep\nmerged &lt;- merged %&gt;%\n  select(all_of(columns_to_keep))\n\n\n\n\nCode\n## Prep data\n\n# List of sensitive variables to exclude\nsensitive_vars &lt;- c(\"mhs2_juvenile_id\", \"dob\", \"assessment_po\")\n## \"unique_id\", \"youth_num_in_cohort\"\n\n# Function to clean variable names\nclean_variable_names &lt;- function(merged) {\n  cleaned_names &lt;- merged %&gt;%\n    names() %&gt;%\n    str_replace_all(\"[^[:alnum:]_]\", \"_\") %&gt;%\n    str_replace_all(\"__+\", \"_\") %&gt;%\n    str_trim() %&gt;%\n    tolower()\n  \n  # Ensure unique names\n  cleaned_names &lt;- make.unique(cleaned_names)\n  \n  names(merged) &lt;- cleaned_names\n  return(merged)\n}\n\n# Function to calculate means and standard deviations for numeric columns\ncalculate_means_sds &lt;- function(merged) {\n  means_sds &lt;- sapply(merged, function(x) if(is.numeric(x)) {\n    mean_value &lt;- round(mean(x, na.rm = TRUE), 2)\n    sd_value &lt;- round(sd(x, na.rm = TRUE), 2)\n    paste0(mean_value, \" (\", sd_value, \")\")\n  } else NA)\n  return(means_sds)\n}\n\n# Function to set variable types based on dlookr diagnostics\nset_variable_types &lt;- function(merged, types) {\n  for (i in 1:nrow(types)) {\n    var &lt;- types$variables[i]\n    type &lt;- types$types[i]\n    if (type == \"numeric\") {\n      merged[[var]] &lt;- as.numeric(merged[[var]])\n    } else if (type == \"factor\") {\n      merged[[var]] &lt;- as.factor(merged[[var]])\n    } else if (type == \"integer\") {\n      merged[[var]] &lt;- as.integer(merged[[var]])\n    } else if (type == \"character\") {\n      merged[[var]] &lt;- as.character(merged[[var]])\n    }\n    # Add other types as necessary\n  }\n  return(merged)\n}\n\n# Function to convert character variables to factors\nconvert_char_to_factor &lt;- function(merged) {\n  merged &lt;- merged %&gt;%\n    mutate(across(where(is.character), as.factor))\n  return(merged)\n}\n\n# Function to calculate the mode for factor variables\ncalculate_modes &lt;- function(merged) {\n  modes &lt;- sapply(merged, function(x) if(is.factor(x)) {\n    mode_value &lt;- names(sort(table(x), decreasing = TRUE)[1])\n    mode_count &lt;- max(table(x))\n    mode_percentage &lt;- (mode_count / length(x)) * 100\n    paste0(mode_value, \" (\", round(mode_percentage, 2), \"%)\")\n  } else NA)\n  return(modes)\n}\n\n# Function to calculate date statistics for POSIXct columns\ncalculate_date_statistics &lt;- function(merged) {\n  date_stats &lt;- sapply(merged, function(x) if(inherits(x, \"POSIXct\")) {\n    min_date &lt;- format(min(x, na.rm = TRUE), \"%Y-%m-%d\")\n    max_date &lt;- format(max(x, na.rm = TRUE), \"%Y-%m-%d\")\n    num_unique_dates &lt;- length(unique(x))\n    median_date &lt;- format(median(x, na.rm = TRUE), \"%Y-%m-%d\")\n    paste0(min_date, \" to \", max_date)\n  } else NA)\n  return(date_stats)\n}\n\n# Function to count the number of levels for factor variables\ncount_levels &lt;- function(merged) {\n  num_levels &lt;- sapply(merged, function(x) if(is.factor(x)) length(levels(x)) else NA)\n  return(num_levels)\n}\n\n# Function to mask sensitive data, only for existing columns\nmask_sensitive_data &lt;- function(merged, sensitive_vars) {\n  existing_sensitive_vars &lt;- intersect(names(merged), sensitive_vars)\n  if(length(existing_sensitive_vars) &gt; 0) {\n    merged &lt;- merged %&gt;%\n      mutate(across(all_of(existing_sensitive_vars), ~ \"Masked\"))\n  }\n  return(merged)\n}\n\n# Apply the cleaning function to the dataset\nmerged_clean &lt;- clean_variable_names(merged)\n\n# Convert character variables to factors\n#merged_clean &lt;- convert_char_to_factor(merged_clean)\n\n# Determine variable types using dlookr\nvariable_types &lt;- diagnose(merged_clean) %&gt;%\n  select(variables, types)\n\n# Set variable types based on dlookr diagnostics\nmerged_clean &lt;- set_variable_types(merged_clean, variable_types)\n\n# Mask sensitive data\nmerged_clean &lt;- mask_sensitive_data(merged_clean, sensitive_vars)\n\n# Create a mapping table of original and cleaned names, excluding sensitive variables\nmapping_table &lt;- tibble(\n  original_name = names(merged),\n  cleaned_name = names(merged_clean)\n) %&gt;%\n  filter(!cleaned_name %in% sensitive_vars)\n\n# Calculate means and standard deviations for numeric columns\nmeans_sds &lt;- calculate_means_sds(merged_clean)\n\n# Calculate modes for factor columns\nmodes &lt;- calculate_modes(merged_clean)\n\n# Calculate date statistics for POSIXct columns\ndate_statistics &lt;- calculate_date_statistics(merged_clean)\n\n# Count the number of levels for factor variables\nnum_levels &lt;- count_levels(merged_clean)\n\n# Combine means, modes, date statistics, and sensitive summaries into separate columns\nmapping_table &lt;- mapping_table %&gt;%\n  mutate(mean_sd = means_sds[!names(means_sds) %in% sensitive_vars],\n         mode = modes[!names(modes) %in% sensitive_vars],\n         date_range = date_statistics[!names(date_statistics) %in% sensitive_vars],\n         num_levels = num_levels[!names(num_levels) %in% sensitive_vars]) %&gt;%\n  left_join(variable_types, by = c(\"cleaned_name\" = \"variables\"))\n\n# Ensure all numeric values in the table are rounded to two decimal places\nmapping_table &lt;- mapping_table %&gt;%\n  mutate(across(where(is.numeric), ~ round(., 2)))\n\n# Reorder the columns so that summary_statistic is before original_name\nmapping_table &lt;- mapping_table %&gt;%\n  select(cleaned_name, mean_sd, mode, date_range, num_levels, original_name, types)"
  },
  {
    "objectID": "index.html#sample-descriptives",
    "href": "index.html#sample-descriptives",
    "title": "SPEP Revalidation",
    "section": "Sample Descriptives",
    "text": "Sample Descriptives\n\nGeneral Data ViewCommunity/ResidentialService TypeRisk ScoresRisk Scores II\n\n\n\n\nThis table shows some general info for full sample (including duplicates), will probably be removed later\n\n\n\nCode\n# Render the table using DT::datatable with tooltips and better formatting\ndatatable(mapping_table, \n          rownames = FALSE,\n          caption = 'Summary Statistics Table',\n          extensions = 'Buttons',\n          options = list(\n            pageLength = 10,\n            scrollX = TRUE,\n            scrollY = \"500px\",\n            dom = 'Bfrtip',\n            buttons = c('copy', 'csv', 'excel', 'pdf', 'print')\n          )) %&gt;%\n  formatStyle(\n    columns = names(mapping_table),\n    valueColumns = \"types\",\n    backgroundColor = styleEqual(c(\"numeric\", \"factor\", \"POSIXct\"), c(\"lightblue\", \"lightgreen\", \"lightpink\"))\n  )\n\n\n\n\n\n\n\n\n\n\n\nThis code creates a basic descriptives table by setting (community vs residential)\n\n\n\nCode\n# Obtain the variable names from the merged dataframe\nvariables &lt;- names(merged_clean)\n\n# Filter variables that include the sequence \"yls\"\nfiltered_variables &lt;- grep(\"yls\", variables, value = TRUE)\n\n# Print the filtered variables\n#print(filtered_variables)\n\n# Create a test dataframe with the specified variables\ntest_df &lt;- merged_clean %&gt;%\n  select(\n    \"age_at_service_start\",\n    \"gender\",\n    \"race\",\n    \"ethnicity\",\n    \"racnicity\",\n    \"age_at_first_referral_date\",\n    #youth_num_in_cohort,\n    #number_of_youth_scoring_low_on_the_yls,\n    #number_of_youth_scoring_moderate_on_the_yls,\n    #number_of_youth_scoring_high_on_the_yls,\n    #number_of_youth_scoring_very_high_on_the_yls,\n    #points_received_risk_level_of_youth_youth_scoring_above_low_on_the_yls,\n    #points_received_risk_level_of_youth_youth_scoring_above_moderate_on_the_yls,\n    setting\n  )\n\n# Create a summary table for the full sample\nfull_sample_summary &lt;- tbl_summary(\n  test_df,\n  by = setting,\n  statistic = list(\n    all_continuous() ~ \"{mean} ({sd})\",\n    all_categorical() ~ \"{n} ({p}%)\"\n  ),\n  missing = \"no\"\n) %&gt;%\n  add_p() %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Variable**\") %&gt;%\n  modify_caption(\"**Descriptive Statistics by Setting**\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"Mean (SD) for continuous variables, N (%) for categorical variables\"\n  )\n\n# Display the summary table\nfull_sample_summary\n\n\n\n\n\n\n\nDescriptive Statistics by Setting\n\n\nVariable\nC, N = 8021\nR, N = 3,1831\np-value2\n\n\n\n\nage_at_service_start\n16.27 (1.53)\n16.82 (1.50)\n&lt;0.001\n\n\ngender\n\n\n&lt;0.001\n\n\n    Female\n199 (25%)\n420 (13%)\n\n\n\n    Male\n603 (75%)\n2,763 (87%)\n\n\n\nrace\n\n\n&lt;0.001\n\n\n    Asian\n3 (0.4%)\n4 (0.1%)\n\n\n\n    Black\n249 (31%)\n1,622 (51%)\n\n\n\n    Multiracial\n35 (4.4%)\n169 (5.3%)\n\n\n\n    Native Hawaiian or Pacific Islander\n0 (0%)\n4 (0.1%)\n\n\n\n    White\n515 (64%)\n1,384 (43%)\n\n\n\nethnicity\n\n\n&lt;0.001\n\n\n    Hispanic\n207 (26%)\n407 (13%)\n\n\n\n    Non-Hispanic\n589 (73%)\n2,765 (87%)\n\n\n\n    Unknown\n6 (0.7%)\n11 (0.3%)\n\n\n\nracnicity\n\n\n\n\n\n    BLACK-NH\n217 (27%)\n1,567 (49%)\n\n\n\n    HISPANIC\n207 (26%)\n407 (13%)\n\n\n\n    OTHER\n24 (3.0%)\n119 (3.7%)\n\n\n\n    UNKNOWN\n6 (0.7%)\n11 (0.3%)\n\n\n\n    WHITE-NH\n348 (43%)\n1,079 (34%)\n\n\n\nage_at_first_referral_date\n14.93 (1.67)\n14.23 (1.84)\n&lt;0.001\n\n\n\n1 Mean (SD) for continuous variables, N (%) for categorical variables\n\n\n2 Wilcoxon rank sum test; Pearson’s Chi-squared test; Fisher’s exact test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis code creates a basic descriptives table for Service Type (full sample)\n\n\n\nCode\n# Create a test dataframe with the specified variables\ntest_df &lt;- merged_clean %&gt;%\n  select(\n    service_type\n  )\n\n# Create a summary table for 'service_type'\nservice_type_summary &lt;- tbl_summary(\n  test_df,\n  statistic = list(\n    all_categorical() ~ \"{n} ({p}%)\"\n  ),\n  missing = \"no\"\n) %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Service Type**\") %&gt;%\n  modify_caption(\"**Distribution of Service Type**\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"N (%) for categorical variables\"\n  )\n\n# Display the summary table\nservice_type_summary\n\n\n\n\n\n\n\nDistribution of Service Type\n\n\nService Type\nN = 3,9851\n\n\n\n\nservice_type\n\n\n\n    AGGRESSION REPLACEMENT TRAINING (ART) COMMUNITY BASED\n76 (2.0%)\n\n\n    AGGRESSION REPLACEMENT TRAINING (ART) RESIDENTIAL PROGRAM\n235 (6.1%)\n\n\n    BEHAVIOR MANAGEMENT\n208 (5.4%)\n\n\n    CHALLENGE PROGRAMS\n52 (1.3%)\n\n\n    COGNITIVE BEHAVIORAL THERAPY\n612 (16%)\n\n\n    FAMILY COUNSELING\n37 (1.0%)\n\n\n    GROUP COUNSELING\n742 (19%)\n\n\n    INDIVIDUAL COUNSELING\n533 (14%)\n\n\n    JOB RELATED TRAINING JOB TRAINING\n163 (4.2%)\n\n\n    JOB RELATED TRAINING VOCATIONAL COUNSELING\n186 (4.8%)\n\n\n    JOB RELATED TRAINING: JOB TRAINING\n19 (0.5%)\n\n\n    MENTORING\n193 (5.0%)\n\n\n    MIXED COUNSELING\n55 (1.4%)\n\n\n    MULTISYSTEMIC THERAPY (MST)\n64 (1.7%)\n\n\n    REMEDIAL ACADEMIC PROGRAM\n296 (7.6%)\n\n\n    RESTITUTION/COMMUNITY SERVICE\n115 (3.0%)\n\n\n    SOCIAL SKILLS TRAINING\n285 (7.4%)\n\n\n\n1 N (%) for categorical variables\n\n\n\n\n\n\n\n\n\n\nThis code creates a basic descriptives table for Service Type (no duplicates)\n\n\n\nCode\n# Convert SERVICE_START_DATE to Date type if it's not already\nmerged$SERVICE_START_DATE &lt;- as.Date(merged$SERVICE_START_DATE, format = \"%Y-%m-%d\")\n\n# Filter to keep the row with the earliest SERVICE_START_DATE for each UNIQUE_ID\nfiltered_df &lt;- merged %&gt;%\n  group_by(UNIQUE_ID) %&gt;%\n  filter(SERVICE_START_DATE == min(SERVICE_START_DATE, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Create a test dataframe with the specified variable\ntest_df &lt;- filtered_df %&gt;%\n  select(Service_Type)\n\n# Create a summary table for 'service_type'\nservice_type_summary &lt;- tbl_summary(\n  test_df,\n  statistic = list(\n    all_categorical() ~ \"{n} ({p}%)\"\n  ),\n  missing = \"no\"\n) %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Service Type**\") %&gt;%\n  modify_caption(\"**Distribution of Service Type**\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"N (%) for categorical variables\"\n  )\n\n# Display the summary table\nservice_type_summary\n\n\n\n\n\n\n\nDistribution of Service Type\n\n\n\n\n\n\nService Type\nN = 2,6601\n\n\n\n\nService_Type\n\n\n\n\n    AGGRESSION REPLACEMENT TRAINING (ART) COMMUNITY BASED\n46 (1.8%)\n\n\n    AGGRESSION REPLACEMENT TRAINING (ART) RESIDENTIAL PROGRAM\n34 (1.3%)\n\n\n    BEHAVIOR MANAGEMENT\n168 (6.6%)\n\n\n    CHALLENGE PROGRAMS\n3 (0.1%)\n\n\n    COGNITIVE BEHAVIORAL THERAPY\n312 (12%)\n\n\n    FAMILY COUNSELING\n33 (1.3%)\n\n\n    GROUP COUNSELING\n548 (21%)\n\n\n    INDIVIDUAL COUNSELING\n427 (17%)\n\n\n    JOB RELATED TRAINING JOB TRAINING\n146 (5.7%)\n\n\n    JOB RELATED TRAINING VOCATIONAL COUNSELING\n95 (3.7%)\n\n\n    JOB RELATED TRAINING: JOB TRAINING\n17 (0.7%)\n\n\n    MENTORING\n184 (7.2%)\n\n\n    MIXED COUNSELING\n34 (1.3%)\n\n\n    MULTISYSTEMIC THERAPY (MST)\n62 (2.4%)\n\n\n    REMEDIAL ACADEMIC PROGRAM\n181 (7.1%)\n\n\n    RESTITUTION/COMMUNITY SERVICE\n79 (3.1%)\n\n\n    SOCIAL SKILLS TRAINING\n185 (7.2%)\n\n\n\n1 N (%) for categorical variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis table shows xxxxxx (full-sample)\n\n\n\nCode\n# # Convert SERVICE_START_DATE to Date type if it's not already\n# merged$SERVICE_START_DATE &lt;- as.Date(merged$SERVICE_START_DATE, format = \"%Y-%m-%d\")\n# \n# # Filter to keep the row with the earliest SERVICE_START_DATE for each UNIQUE_ID\n# filtered_df &lt;- merged %&gt;%\n#   group_by(UNIQUE_ID) %&gt;%\n#   filter(SERVICE_START_DATE == min(SERVICE_START_DATE, na.rm = TRUE)) %&gt;%\n#   ungroup()\n\n# Convert Total_Risk_Desc to a factor\nmerged$Total_Risk_Desc &lt;- as.factor(merged$Total_Risk_Desc)\n\n# Create a test dataframe with the specified variables\ntest_df &lt;- merged %&gt;%\n  select(Total_Risk_Desc, TOTALSCORE)\n\n# Create summary table for 'Total_Risk_Desc'\ntotal_risk_desc_summary &lt;- tbl_summary(\n  test_df %&gt;% select(Total_Risk_Desc),\n  statistic = list(\n    all_categorical() ~ \"{n} ({p}%)\"\n  ),\n  missing = \"no\"\n) %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Total Risk Description**\") %&gt;%\n  modify_caption(\"**Distribution of Total Risk Description**\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"N (%) for categorical variables\"\n  )\n\n# Create summary table for 'TOTALSCORE'\ntotalscore_summary &lt;- tbl_summary(\n  test_df %&gt;% select(TOTALSCORE),\n  statistic = list(\n    all_continuous() ~ \"{mean} ({sd})\"\n  ),\n  missing = \"no\"\n) %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Total Score**\") %&gt;%\n  modify_caption(\"**Descriptive Statistics for Total Score**\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"Mean (SD) for continuous variables\"\n  )\n\n# Convert summary tables to gt tables\ntotal_risk_desc_gt &lt;- as_gt(total_risk_desc_summary)\ntotalscore_gt &lt;- as_gt(totalscore_summary)\n\n# Display the summary tables side by side using gt\n#library(gt)\n#gt::gtsave(total_risk_desc_gt, \"total_risk_desc_summary.html\")\n#gt::gtsave(totalscore_gt, \"totalscore_summary.html\")\n\ntotal_risk_desc_gt\n\n\n\n\n\n\n\nDistribution of Total Risk Description\n\n\n\n\n\n\nTotal Risk Description\nN = 3,9851\n\n\n\n\nTotal_Risk_Desc\n\n\n\n\n    High\n1,265 (32%)\n\n\n    Low\n491 (12%)\n\n\n    Moderate\n2,165 (54%)\n\n\n    Very High\n64 (1.6%)\n\n\n\n1 N (%) for categorical variables\n\n\n\n\n\n\n\n\n\nCode\ntotalscore_gt\n\n\n\n\n\n\n\nDescriptive Statistics for Total Score\n\n\nTotal Score\nN = 3,9851\n\n\n\n\nTOTALSCORE\n18 (7)\n\n\n\n1 Mean (SD) for continuous variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis code creates a basic descriptives table for Service Type (no duplicates)\n\n\n\nCode\n# Convert SERVICE_START_DATE to Date type if it's not already\nmerged$SERVICE_START_DATE &lt;- as.Date(merged$SERVICE_START_DATE, format = \"%Y-%m-%d\")\n\n# Filter to keep the row with the earliest SERVICE_START_DATE for each UNIQUE_ID\nfiltered_df &lt;- merged %&gt;%\n  group_by(UNIQUE_ID) %&gt;%\n  filter(SERVICE_START_DATE == min(SERVICE_START_DATE, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Convert Total_Risk_Desc to a factor\nfiltered_df$Total_Risk_Desc &lt;- as.factor(filtered_df$Total_Risk_Desc)\n\n# Create a test dataframe with the specified variables\ntest_df &lt;- filtered_df %&gt;%\n  select(Total_Risk_Desc, TOTALSCORE)\n\n# Create summary table for 'Total_Risk_Desc'\ntotal_risk_desc_summary &lt;- tbl_summary(\n  test_df %&gt;% select(Total_Risk_Desc),\n  statistic = list(\n    all_categorical() ~ \"{n} ({p}%)\"\n  ),\n  missing = \"no\"\n) %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Total Risk Description**\") %&gt;%\n  modify_caption(\"**Distribution of Total Risk Description**\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"N (%) for categorical variables\"\n  )\n\n# Create summary table for 'TOTALSCORE'\ntotalscore_summary &lt;- tbl_summary(\n  test_df %&gt;% select(TOTALSCORE),\n  statistic = list(\n    all_continuous() ~ \"{mean} ({sd})\"\n  ),\n  missing = \"no\"\n) %&gt;%\n  bold_labels() %&gt;%\n  modify_header(label ~ \"**Total Score**\") %&gt;%\n  modify_caption(\"**Descriptive Statistics for Total Score**\") %&gt;%\n  modify_footnote(\n    all_stat_cols() ~ \"Mean (SD) for continuous variables\"\n  )\n\n# Convert summary tables to gt tables\ntotal_risk_desc_gt &lt;- as_gt(total_risk_desc_summary)\ntotalscore_gt &lt;- as_gt(totalscore_summary)\n\n# Display the summary tables side by side using gt\n#library(gt)\n#gt::gtsave(total_risk_desc_gt, \"total_risk_desc_summary.html\")\n#gt::gtsave(totalscore_gt, \"totalscore_summary.html\")\n\ntotal_risk_desc_gt\n\n\n\n\n\n\n\nDistribution of Total Risk Description\n\n\n\n\n\n\nTotal Risk Description\nN = 2,6601\n\n\n\n\nTotal_Risk_Desc\n\n\n\n\n    High\n775 (29%)\n\n\n    Low\n379 (14%)\n\n\n    Moderate\n1,469 (55%)\n\n\n    Very High\n37 (1.4%)\n\n\n\n1 N (%) for categorical variables\n\n\n\n\n\n\n\n\n\nCode\ntotalscore_gt\n\n\n\n\n\n\n\nDescriptive Statistics for Total Score\n\n\nTotal Score\nN = 2,6601\n\n\n\n\nTOTALSCORE\n17 (7)\n\n\n\n1 Mean (SD) for continuous variables"
  },
  {
    "objectID": "index.html#cohort-descriptives",
    "href": "index.html#cohort-descriptives",
    "title": "SPEP Revalidation",
    "section": "Cohort Descriptives",
    "text": "Cohort Descriptives\n\nCohort TableCohort Stats\n\n\n\n\nThis table shows descriptives by cohort for YLS scores\n\n\n\nCode\n# Step 2.3: Create and format the table using kable and kableExtra with enhancements\nformatted_table &lt;- contingency_df %&gt;%\n  kable(\"html\", col.names = c(\"Index\", \"Cohort\", \"Count\", \"Mean(SD)\", \"High\", \"Low\", \"Moderate\", \"Very High\"), caption = \"Cohort YLS Table\") %&gt;%\n  kable_styling(full_width = F, position = \"center\", bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#4CAF50\") %&gt;%  # Header row styling\n  row_spec(1:nrow(contingency_df), background = c(\"#f2f2f2\", \"white\")) %&gt;%  # Alternating row colors\n  column_spec(1, bold = TRUE, color = \"blue\") %&gt;%  # Highlight the Index column\n  column_spec(3, background = \"#ffebcc\") %&gt;%  # Highlight the Count column\n  add_header_above(c(\"Cohort Information\" = 3, \"YLS Total\" = 1, \"YLS Risk Levels\" = 4)) %&gt;%  # Multi-level header\n  footnote(general = \"This table provides xxxxx.\")  # Add a footnote\n\n# Display the formatted table\nformatted_table\n\n\n\n\n\nCohort YLS Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nCohort Information\n\n\nYLS Total\n\n\nYLS Risk Levels\n\n\n\nIndex\nCohort\nCount\nMean(SD)\nHigh\nLow\nModerate\nVery High\n\n\n\n\n1\n0303-\n150\n18.47 (6.43)\n49 (33%)\n11 (7%)\n87 (58%)\n3 (2%)\n\n\n2\n0121-\n113\n19.79 (7.24)\n40 (35%)\n10 (9%)\n56 (50%)\n7 (6%)\n\n\n3\n0135-\n107\n18.78 (6.84)\n40 (37%)\n11 (10%)\n55 (51%)\n1 (1%)\n\n\n4\n0133-\n105\n18.76 (6.90)\n40 (38%)\n11 (10%)\n53 (50%)\n1 (1%)\n\n\n5\n0111-\n103\n19.03 (6.43)\n36 (35%)\n7 (7%)\n57 (55%)\n3 (3%)\n\n\n6\n0042-\n95\n19.67 (5.33)\n39 (41%)\n5 (5%)\n50 (53%)\n1 (1%)\n\n\n7\n0140-\n81\n19.56 (7.25)\n28 (35%)\n8 (10%)\n41 (51%)\n4 (5%)\n\n\n8\n0210-\n79\n15.90 (5.26)\n11 (14%)\n6 (8%)\n62 (78%)\n0 (0%)\n\n\n9\n0222-\n79\n17.22 (7.67)\n24 (30%)\n12 (15%)\n42 (53%)\n1 (1%)\n\n\n10\n0036-\n78\n19.03 (5.98)\n27 (35%)\n6 (8%)\n44 (56%)\n1 (1%)\n\n\n11\n0304-\n76\n15.84 (7.43)\n23 (30%)\n18 (24%)\n35 (46%)\n0 (0%)\n\n\n12\n0157-\n74\n20.01 (5.46)\n28 (38%)\n3 (4%)\n43 (58%)\n0 (0%)\n\n\n13\n0034-\n68\n19.51 (6.07)\n26 (38%)\n5 (7%)\n36 (53%)\n1 (1%)\n\n\n14\n0189-\n62\n16.58 (7.47)\n19 (31%)\n13 (21%)\n30 (48%)\n0 (0%)\n\n\n15\n0091-\n58\n19.33 (5.49)\n20 (34%)\n4 (7%)\n34 (59%)\n0 (0%)\n\n\n16\n0085-\n57\n18.05 (6.93)\n19 (33%)\n8 (14%)\n29 (51%)\n1 (2%)\n\n\n17\n0087-\n57\n18.05 (6.93)\n19 (33%)\n8 (14%)\n29 (51%)\n1 (2%)\n\n\n18\n0035-\n55\n17.36 (6.68)\n17 (31%)\n9 (16%)\n28 (51%)\n1 (2%)\n\n\n19\n0272-\n52\n20.46 (4.93)\n24 (46%)\n2 (4%)\n26 (50%)\n0 (0%)\n\n\n20\n0045-\n50\n16.88 (5.79)\n13 (26%)\n4 (8%)\n33 (66%)\n0 (0%)\n\n\n21\n0114-\n50\n19.58 (7.46)\n17 (34%)\n5 (10%)\n25 (50%)\n3 (6%)\n\n\n22\n0115-\n50\n19.58 (7.46)\n17 (34%)\n5 (10%)\n25 (50%)\n3 (6%)\n\n\n23\n0141-\n50\n19.58 (7.46)\n17 (34%)\n5 (10%)\n25 (50%)\n3 (6%)\n\n\n24\n0014-\n48\n19.06 (7.38)\n18 (38%)\n6 (12%)\n22 (46%)\n2 (4%)\n\n\n25\n0007-\n47\n19.06 (5.73)\n22 (47%)\n2 (4%)\n21 (45%)\n2 (4%)\n\n\n26\n0013-\n47\n18.98 (7.43)\n17 (36%)\n6 (13%)\n22 (47%)\n2 (4%)\n\n\n27\n0112-\n47\n17.28 (6.31)\n13 (28%)\n4 (9%)\n30 (64%)\n0 (0%)\n\n\n28\n0010-\n46\n19.04 (5.78)\n22 (48%)\n2 (4%)\n20 (43%)\n2 (4%)\n\n\n29\n0011-\n46\n18.91 (7.57)\n17 (37%)\n6 (13%)\n21 (46%)\n2 (4%)\n\n\n30\n0012-\n46\n18.76 (7.94)\n19 (41%)\n8 (17%)\n17 (37%)\n2 (4%)\n\n\n31\n0108-\n46\n17.22 (6.37)\n13 (28%)\n4 (9%)\n29 (63%)\n0 (0%)\n\n\n32\n0009-\n45\n19.20 (5.75)\n22 (49%)\n2 (4%)\n19 (42%)\n2 (4%)\n\n\n33\n0005-\n42\n19.00 (7.35)\n17 (40%)\n4 (10%)\n20 (48%)\n1 (2%)\n\n\n34\n0319-\n42\n20.74 (5.74)\n19 (45%)\n0 (0%)\n22 (52%)\n1 (2%)\n\n\n35\n0004-\n41\n18.88 (7.40)\n16 (39%)\n4 (10%)\n20 (49%)\n1 (2%)\n\n\n36\n0006-\n41\n19.44 (6.86)\n17 (41%)\n3 (7%)\n20 (49%)\n1 (2%)\n\n\n37\n0188-\n41\n16.54 (7.98)\n15 (37%)\n9 (22%)\n17 (41%)\n0 (0%)\n\n\n38\n0190-\n41\n16.54 (7.98)\n15 (37%)\n9 (22%)\n17 (41%)\n0 (0%)\n\n\n39\n0191-\n41\n16.54 (7.98)\n15 (37%)\n9 (22%)\n17 (41%)\n0 (0%)\n\n\n40\n0223-\n41\n17.34 (6.57)\n10 (24%)\n5 (12%)\n26 (63%)\n0 (0%)\n\n\n41\n0343-\n40\n15.72 (6.30)\n5 (12%)\n7 (18%)\n28 (70%)\n0 (0%)\n\n\n42\n0345-\n40\n15.72 (6.30)\n5 (12%)\n7 (18%)\n28 (70%)\n0 (0%)\n\n\n43\n0003-\n39\n19.33 (6.93)\n16 (41%)\n3 (8%)\n19 (49%)\n1 (3%)\n\n\n44\n0187-\n39\n18.33 (6.38)\n14 (36%)\n5 (13%)\n20 (51%)\n0 (0%)\n\n\n45\n0037-\n37\n20.38 (5.42)\n14 (38%)\n1 (3%)\n22 (59%)\n0 (0%)\n\n\n46\n0159-\n36\n18.50 (5.22)\n16 (44%)\n2 (6%)\n17 (47%)\n1 (3%)\n\n\n47\n0320-\n36\n19.06 (4.39)\n11 (31%)\n1 (3%)\n24 (67%)\n0 (0%)\n\n\n48\n0350-\n34\n10.44 (7.65)\n4 (12%)\n15 (44%)\n15 (44%)\n0 (0%)\n\n\n49\n0310-\n33\n10.76 (5.32)\n1 (3%)\n14 (42%)\n18 (55%)\n0 (0%)\n\n\n50\n0344-\n33\n15.55 (6.49)\n4 (12%)\n7 (21%)\n22 (67%)\n0 (0%)\n\n\n51\n0302-\n32\n18.78 (6.59)\n11 (34%)\n2 (6%)\n18 (56%)\n1 (3%)\n\n\n52\n0321-\n32\n6.91 (5.59)\n1 (3%)\n22 (69%)\n9 (28%)\n0 (0%)\n\n\n53\n0120-\n31\n19.52 (7.02)\n11 (35%)\n3 (10%)\n16 (52%)\n1 (3%)\n\n\n54\n0139-\n31\n19.52 (7.02)\n11 (35%)\n3 (10%)\n16 (52%)\n1 (3%)\n\n\n55\n0067-\n30\n14.33 (6.79)\n5 (17%)\n8 (27%)\n17 (57%)\n0 (0%)\n\n\n56\n0160-\n30\n13.33 (8.22)\n4 (13%)\n8 (27%)\n18 (60%)\n0 (0%)\n\n\n57\n0236-\n30\n16.87 (6.58)\n4 (13%)\n3 (10%)\n22 (73%)\n1 (3%)\n\n\n58\n0171-\n29\n18.62 (6.54)\n10 (34%)\n3 (10%)\n16 (55%)\n0 (0%)\n\n\n59\n0172-\n29\n18.62 (6.54)\n10 (34%)\n3 (10%)\n16 (55%)\n0 (0%)\n\n\n60\n0311-\n29\n11.17 (5.38)\n1 (3%)\n12 (41%)\n16 (55%)\n0 (0%)\n\n\n61\n0044-\n28\n15.75 (5.79)\n4 (14%)\n2 (7%)\n22 (79%)\n0 (0%)\n\n\n62\n0041-\n27\n20.44 (4.32)\n13 (48%)\n0 (0%)\n14 (52%)\n0 (0%)\n\n\n63\n0024-\n24\n17.79 (6.55)\n8 (33%)\n3 (12%)\n13 (54%)\n0 (0%)\n\n\n64\n0057-\n24\n17.25 (5.29)\n8 (33%)\n1 (4%)\n15 (62%)\n0 (0%)\n\n\n65\n0351-\n24\n13.67 (4.43)\n1 (4%)\n4 (17%)\n19 (79%)\n0 (0%)\n\n\n66\n0158-\n22\n15.82 (5.22)\n3 (14%)\n4 (18%)\n15 (68%)\n0 (0%)\n\n\n67\n0339-\n22\n8.55 (6.54)\n1 (5%)\n13 (59%)\n8 (36%)\n0 (0%)\n\n\n68\n0168-\n21\n15.90 (7.25)\n1 (5%)\n3 (14%)\n16 (76%)\n1 (5%)\n\n\n69\n0233-\n21\n14.86 (5.04)\n3 (14%)\n4 (19%)\n14 (67%)\n0 (0%)\n\n\n70\n0322-\n21\n16.19 (4.79)\n5 (24%)\n2 (10%)\n14 (67%)\n0 (0%)\n\n\n71\n0113-\n19\n14.74 (5.45)\n1 (5%)\n2 (11%)\n16 (84%)\n0 (0%)\n\n\n72\n0312-\n19\n18.79 (4.61)\n6 (32%)\n0 (0%)\n13 (68%)\n0 (0%)\n\n\n73\n0313-\n19\n18.79 (4.61)\n6 (32%)\n0 (0%)\n13 (68%)\n0 (0%)\n\n\n74\n0314-\n19\n18.79 (4.61)\n6 (32%)\n0 (0%)\n13 (68%)\n0 (0%)\n\n\n75\n0316-\n19\n18.79 (4.61)\n6 (32%)\n0 (0%)\n13 (68%)\n0 (0%)\n\n\n76\n0317-\n19\n18.79 (4.61)\n6 (32%)\n0 (0%)\n13 (68%)\n0 (0%)\n\n\n77\n0325-\n19\n19.79 (4.42)\n8 (42%)\n0 (0%)\n11 (58%)\n0 (0%)\n\n\n78\n0225-\n18\n19.28 (4.74)\n11 (61%)\n0 (0%)\n7 (39%)\n0 (0%)\n\n\n79\n0244-\n18\n14.50 (7.38)\n3 (17%)\n3 (17%)\n12 (67%)\n0 (0%)\n\n\n80\n0025-\n17\n11.29 (5.11)\n0 (0%)\n7 (41%)\n10 (59%)\n0 (0%)\n\n\n81\n0227-\n17\n19.24 (5.94)\n9 (53%)\n1 (6%)\n7 (41%)\n0 (0%)\n\n\n82\n0300-\n17\n21.00 (7.12)\n8 (47%)\n0 (0%)\n9 (53%)\n0 (0%)\n\n\n83\n0349-\n17\n19.24 (5.94)\n10 (59%)\n1 (6%)\n6 (35%)\n0 (0%)\n\n\n84\n0334-\n16\n14.25 (4.58)\n2 (12%)\n1 (6%)\n13 (81%)\n0 (0%)\n\n\n85\n0318-\n15\n14.60 (6.15)\n2 (13%)\n3 (20%)\n10 (67%)\n0 (0%)\n\n\n86\n0305-\n14\n17.00 (6.69)\n5 (36%)\n2 (14%)\n7 (50%)\n0 (0%)\n\n\n87\n0307-\n14\n17.00 (6.69)\n5 (36%)\n2 (14%)\n7 (50%)\n0 (0%)\n\n\n88\n0315-\n14\n18.79 (4.79)\n5 (36%)\n0 (0%)\n9 (64%)\n0 (0%)\n\n\n89\n0324-\n14\n13.71 (5.22)\n1 (7%)\n2 (14%)\n11 (79%)\n0 (0%)\n\n\n90\n0329-\n14\n14.57 (6.98)\n2 (14%)\n2 (14%)\n10 (71%)\n0 (0%)\n\n\n91\n0347-\n14\n14.14 (6.57)\n2 (14%)\n2 (14%)\n10 (71%)\n0 (0%)\n\n\n92\n0277-\n13\n18.31 (5.86)\n3 (23%)\n0 (0%)\n9 (69%)\n1 (8%)\n\n\n93\n0323-\n13\n17.77 (5.04)\n5 (38%)\n0 (0%)\n8 (62%)\n0 (0%)\n\n\n94\n0008-\n12\n20.42 (6.69)\n6 (50%)\n0 (0%)\n5 (42%)\n1 (8%)\n\n\n95\n0099-\n12\n18.50 (7.18)\n7 (58%)\n2 (17%)\n3 (25%)\n0 (0%)\n\n\n96\n0101-\n12\n18.50 (7.18)\n7 (58%)\n2 (17%)\n3 (25%)\n0 (0%)\n\n\n97\n0102-\n12\n19.17 (5.59)\n4 (33%)\n0 (0%)\n8 (67%)\n0 (0%)\n\n\n98\n0109-\n12\n21.25 (6.33)\n5 (42%)\n0 (0%)\n7 (58%)\n0 (0%)\n\n\n99\n0332-\n12\n19.67 (8.24)\n5 (42%)\n1 (8%)\n5 (42%)\n1 (8%)\n\n\n100\n0098-\n11\n18.64 (7.51)\n7 (64%)\n2 (18%)\n2 (18%)\n0 (0%)\n\n\n101\n0100-\n11\n19.27 (6.99)\n7 (64%)\n2 (18%)\n2 (18%)\n0 (0%)\n\n\n102\n0137-\n11\n17.73 (7.68)\n4 (36%)\n2 (18%)\n5 (45%)\n0 (0%)\n\n\n103\n0151-\n11\n10.73 (6.71)\n1 (9%)\n4 (36%)\n6 (55%)\n0 (0%)\n\n\n104\n0309-\n11\n17.91 (5.65)\n4 (36%)\n0 (0%)\n7 (64%)\n0 (0%)\n\n\n105\n0346-\n11\n10.00 (4.31)\n0 (0%)\n5 (45%)\n6 (55%)\n0 (0%)\n\n\n106\n0357-\n11\n16.55 (9.16)\n2 (18%)\n2 (18%)\n6 (55%)\n1 (9%)\n\n\n107\n0235-\n10\n16.20 (3.52)\n0 (0%)\n0 (0%)\n10 (100%)\n0 (0%)\n\n\n108\n0333-\n10\n11.40 (6.31)\n1 (10%)\n4 (40%)\n5 (50%)\n0 (0%)\n\n\n109\n0348-\n10\n18.20 (4.24)\n5 (50%)\n1 (10%)\n4 (40%)\n0 (0%)\n\n\n110\n0335-\n9\n11.22 (7.73)\n1 (11%)\n4 (44%)\n4 (44%)\n0 (0%)\n\n\n111\n0337-\n9\n11.22 (7.73)\n1 (11%)\n4 (44%)\n4 (44%)\n0 (0%)\n\n\n112\n0338-\n9\n11.22 (7.73)\n1 (11%)\n4 (44%)\n4 (44%)\n0 (0%)\n\n\n\nNote: \n\n\n\n\n\n\n\n\n\n This table provides xxxxx.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis table shows the correlation between cohort size and YLS scores\n\n\n\nCode\n# Calculate the total count for each cohort\ncount_df &lt;- merged %&gt;%\n  group_by(Cohort) %&gt;%\n  summarize(Count = n())\n\n# Calculate the mean totalscore for each cohort\nmean_totalscore_df &lt;- merged %&gt;%\n  group_by(Cohort) %&gt;%\n  summarize(TOTALSCORE = mean(TOTALSCORE, na.rm = TRUE))\n\n# Merge the count and mean totalscore data frames\nmerged_df &lt;- left_join(count_df, mean_totalscore_df, by = \"Cohort\")\n\n# Perform a correlation test between Count and Mean_TOTALSCORE\ncorrelation_test &lt;- cor.test(merged_df$Count, merged_df$TOTALSCORE, use = \"complete.obs\")\n\n# Extract the sample size, correlation coefficient, and p-value\nsample_size &lt;- length(na.omit(merged_df$Count))\ncorrelation_coefficient &lt;- correlation_test$estimate\np_value &lt;- correlation_test$p.value\n\n# Print the results\ncat(\"Sample Size:\", sample_size, \"\\n\")\n\n\nSample Size: 112 \n\n\nCode\ncat(\"Correlation Coefficient:\", correlation_coefficient, \"\\n\")\n\n\nCorrelation Coefficient: 0.2846035 \n\n\nCode\ncat(\"P-value:\", p_value, \"\\n\")\n\n\nP-value: 0.002355382"
  }
]